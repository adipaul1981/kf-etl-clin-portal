args=[
    "re_000001",
    "SD_Z6MWD3H0"
]
sources=[
    {
        format=PARQUET
        id="normalized_researchstudy"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/researchstudy/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_researchstudy"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_patient"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/patient/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_patient"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_documentreference"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/documentreference/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_documentreference"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_specimen"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/specimen/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_specimen"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_condition"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/condition/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_condition"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_observation"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/observation/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_observation"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="es_index_participant_centric"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/es_index/fhir/researchstudy/study=SD_Z6MWD3H0"
        readoptions {}
        storageid=es_index
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    }
]
sparkconf {
    "spark.databricks.delta.retentionDurationCheck.enabled"="false"
    "spark.delta.merge.repartitionBeforeWrite"="true"
    "spark.hadoop.fs.s3a.access.key"=minioadmin
    "spark.hadoop.fs.s3a.endpoint"="http://127.0.0.1:9000"
    "spark.hadoop.fs.s3a.impl"="org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access"="true"
    "spark.hadoop.fs.s3a.secret.key"=minioadmin
    "spark.master"=local
    "spark.sql.catalog.spark_catalog"="org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.sql.extensions"="io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.legacy.timeParserPolicy"=CORRECTED
    "spark.sql.mapKeyDedupPolicy"="LAST_WIN"
}
storages=[
    {
        filesystem=S3
        id=output
        path="s3a://output"
    },
    {
        filesystem=S3
        id=es_index
        path="s3a://es_index"
    }
]
